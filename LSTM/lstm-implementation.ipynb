{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ef3a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    no_of_neurons = 128\n",
    "    block_size = 32\n",
    "    batch_size = 32\n",
    "    dropout = 0.1\n",
    "    epoch = 10\n",
    "    max_lr = 1e-4\n",
    "    embedding_dims = 1  # since we're using scalar sequences like sin(t)\n",
    "    total_samples = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f042804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.X = torch.stack([data[i:i+seq_len] for i in range(len(data) - seq_len)])\n",
    "        self.y = data[seq_len:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(-1), self.y[idx].unsqueeze(-1)  # shape: (seq_len, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06540030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ForgetGate(nn.Module):\n",
    "    \"\"\"custom forget gate for LSTM\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(ModelArgs.no_of_neurons + ModelArgs.embedding_dims, ModelArgs.no_of_neurons)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        xh = torch.cat([x,h_prev], dim=1)\n",
    "        return torch.sigmoid(self.linear(xh))\n",
    "    \n",
    "class InputGate(nn.Module):\n",
    "    \"\"\"custom input gate for LSTM\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i_linear = nn.Linear(ModelArgs.no_of_neurons + ModelArgs.embedding_dims, ModelArgs.no_of_neurons)\n",
    "        self.c_linear = nn.Linear(ModelArgs.no_of_neurons + ModelArgs.embedding_dims, ModelArgs.no_of_neurons)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        xh = torch.cat([x, h_prev], dim=1)\n",
    "        i_t = torch.sigmoid(self.i_linear(xh))  # input gate\n",
    "        c_tilde = torch.tanh(self.c_linear(xh))     # candidate cell state\n",
    "        return i_t, c_tilde\n",
    "\n",
    "class OutputGate(nn.Module):\n",
    "    \"\"\"custom output gate for LSTM\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.o_linear = nn.Linear(ModelArgs.no_of_neurons + ModelArgs.embedding_dims, ModelArgs.no_of_neurons)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        xh = torch.cat([x, h_prev], dim=1)\n",
    "        o_t = torch.sigmoid(self.o_linear(xh))  # output gate\n",
    "        return o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4b05799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBlock(nn.Module):\n",
    "    \"\"\"custom LSTM Block\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_gate = InputGate()\n",
    "        self.forget_gate = ForgetGate()\n",
    "        self.output_gate = OutputGate()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,D = x.shape\n",
    "        h_t = torch.zeros(B, ModelArgs.no_of_neurons, device=x.device)\n",
    "        c_t = torch.zeros(B, ModelArgs.no_of_neurons, device=x.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = x[:, t, :]\n",
    "            f_t = self.forget_gate(x_t, h_t)\n",
    "            i_t, c_tilde = self.input_gate(x_t, h_t)\n",
    "            o_t = self.output_gate(x_t, h_t)\n",
    "            \n",
    "            c_t = f_t * c_t + i_t * c_tilde  # update cell state\n",
    "            h_t = o_t * torch.tanh(c_t)       # update hidden state\n",
    "            outputs.append(h_t.unsqueeze(1))  # store hidden state for each time step\n",
    "        return torch.cat(outputs, dim=1)  # concatenate hidden states\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b602931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"custom LSTM model\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the LSTM model with a custom LSTM block and dropout.\"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = LSTMBlock()\n",
    "        self.dropout = nn.Dropout(ModelArgs.dropout)\n",
    "        self.output = nn.Linear(ModelArgs.no_of_neurons, 1) #predict next value in sequence\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the LSTM model.\"\"\"\n",
    "        x = x.to(ModelArgs.device)\n",
    "        out = self.lstm(x)  # shape: (B, T, no_of_neurons)\n",
    "        out = self.dropout(out[:,-1,:])  # apply dropout, take the last time step\n",
    "        out = self.output(out)  # shape: (B, T, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de699bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 0.0367 | Val Loss: 0.0002\n",
      "Epoch 001 | Train Loss: 0.0010 | Val Loss: 0.0003\n",
      "Epoch 002 | Train Loss: 0.0009 | Val Loss: 0.0002\n",
      "Epoch 003 | Train Loss: 0.0008 | Val Loss: 0.0002\n",
      "Epoch 004 | Train Loss: 0.0007 | Val Loss: 0.0001\n",
      "Epoch 005 | Train Loss: 0.0007 | Val Loss: 0.0001\n",
      "Epoch 006 | Train Loss: 0.0006 | Val Loss: 0.0001\n",
      "Epoch 007 | Train Loss: 0.0006 | Val Loss: 0.0001\n",
      "Epoch 008 | Train Loss: 0.0006 | Val Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/14 12:56:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/14 12:56:02 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 | Train Loss: 0.0006 | Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/14 12:56:06 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/07/14 12:56:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Synthetic Dataset (can be replaced)\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Generate simple data (sinusoidal or linear)\n",
    "def generate_data():\n",
    "    t = torch.linspace(0, 100, ModelArgs.total_samples + ModelArgs.block_size, device=ModelArgs.device)\n",
    "    data = torch.sin(t)  # or t for linear\n",
    "    X = torch.stack([data[i:i+ModelArgs.block_size] for i in range(ModelArgs.total_samples)])\n",
    "    y = data[ModelArgs.block_size:]\n",
    "    return X.unsqueeze(-1), y.unsqueeze(-1)\n",
    "\n",
    "# Setup\n",
    "X, y = generate_data()\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:], y[train_size:]\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=ModelArgs.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=ModelArgs.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Model\n",
    "model = LSTM().to(ModelArgs.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=ModelArgs.max_lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"Custom_LSTM_Experiment\")\n",
    "with mlflow.start_run(run_name=\"custom_lstm_run\"):\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"no_of_neurons\": ModelArgs.no_of_neurons,\n",
    "        \"block_size\": ModelArgs.block_size,\n",
    "        \"batch_size\": ModelArgs.batch_size,\n",
    "        \"dropout\": ModelArgs.dropout,\n",
    "        \"epochs\": ModelArgs.epoch,\n",
    "        \"learning_rate\": ModelArgs.max_lr,\n",
    "    })\n",
    "\n",
    "    for epoch in range(ModelArgs.epoch):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                y_pred = model(xb)\n",
    "                loss = criterion(y_pred, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Val Loss\": val_loss\n",
    "        }, step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
