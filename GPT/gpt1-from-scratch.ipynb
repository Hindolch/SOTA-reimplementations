{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-22T17:26:43.473135Z",
     "iopub.status.busy": "2025-06-22T17:26:43.472880Z",
     "iopub.status.idle": "2025-06-22T17:30:01.985207Z",
     "shell.execute_reply": "2025-06-22T17:30:01.984471Z",
     "shell.execute_reply.started": "2025-06-22T17:26:43.473113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torch-2.7.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.61 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.57 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.57 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.7.1.26 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.3.14 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.41 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.55 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.2.55 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.7.53 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.55 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.61 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.0.11 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torchvision-0.22.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torchaudio-2.7.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading https://download.pytorch.org/whl/cu128/torch-2.7.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl (1039.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 GB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl (609.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m609.6/609.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl (726.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m726.9/726.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl (260.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.4/260.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (292.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/torchvision-0.22.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/torchaudio-2.7.1%2Bcu128-cp311-cp311-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu128 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.3.14 nvidia-cuda-cupti-cu12-12.8.57 nvidia-cuda-nvrtc-cu12-12.8.61 nvidia-cuda-runtime-cu12-12.8.57 nvidia-cudnn-cu12-9.7.1.26 nvidia-cufft-cu12-11.3.3.41 nvidia-cufile-cu12-1.13.0.11 nvidia-curand-cu12-10.3.9.55 nvidia-cusolver-cu12-11.7.2.55 nvidia-cusparse-cu12-12.5.7.53 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.8.55 sympy-1.13.3 torch-2.7.1+cu128 torchaudio-2.7.1+cu128 torchvision-0.22.1+cu128 triton-3.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T17:30:01.986976Z",
     "iopub.status.busy": "2025-06-22T17:30:01.986706Z",
     "iopub.status.idle": "2025-06-22T17:30:02.338802Z",
     "shell.execute_reply": "2025-06-22T17:30:02.337930Z",
     "shell.execute_reply.started": "2025-06-22T17:30:01.986941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-22 17:30:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2025-06-22 17:30:02 (20.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T17:52:34.322824Z",
     "iopub.status.busy": "2025-06-22T17:52:34.322193Z",
     "iopub.status.idle": "2025-06-22T17:52:37.475204Z",
     "shell.execute_reply": "2025-06-22T17:52:37.474220Z",
     "shell.execute_reply.started": "2025-06-22T17:52:34.322801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T17:59:36.311651Z",
     "iopub.status.busy": "2025-06-22T17:59:36.310928Z",
     "iopub.status.idle": "2025-06-22T18:16:33.409047Z",
     "shell.execute_reply": "2025-06-22T18:16:33.408384Z",
     "shell.execute_reply.started": "2025-06-22T17:59:36.311624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.684497 M parameters\n",
      "step 0: train loss 10.9975, val loss 11.0111\n",
      "step 200: train loss 6.0289, val loss 6.1651\n",
      "step 400: train loss 5.5027, val loss 5.7814\n",
      "step 600: train loss 5.1099, val loss 5.4410\n",
      "step 800: train loss 4.8735, val loss 5.2475\n",
      "step 1000: train loss 4.7187, val loss 5.1395\n",
      "step 1200: train loss 4.5791, val loss 5.0175\n",
      "step 1400: train loss 4.4953, val loss 4.9923\n",
      "step 1600: train loss 4.4139, val loss 4.9608\n",
      "step 1800: train loss 4.3685, val loss 4.9090\n",
      "step 2000: train loss 4.3003, val loss 4.8834\n",
      "step 2200: train loss 4.2225, val loss 4.8730\n",
      "step 2400: train loss 4.1729, val loss 4.8911\n",
      "step 2600: train loss 4.1224, val loss 4.8408\n",
      "step 2800: train loss 4.0936, val loss 4.8364\n",
      "step 3000: train loss 4.0472, val loss 4.8851\n",
      "step 3200: train loss 3.9832, val loss 4.8978\n",
      "step 3400: train loss 3.9674, val loss 4.8339\n",
      "step 3600: train loss 3.9440, val loss 4.8383\n",
      "step 3800: train loss 3.8949, val loss 4.8824\n",
      "step 4000: train loss 3.8877, val loss 4.8779\n",
      "step 4200: train loss 3.8454, val loss 4.8342\n",
      "step 4400: train loss 3.8379, val loss 4.8774\n",
      "step 4600: train loss 3.7913, val loss 4.8789\n",
      "step 4800: train loss 3.7748, val loss 4.9321\n",
      "step 5000: train loss 3.7497, val loss 4.9294\n",
      "step 5200: train loss 3.7289, val loss 4.9292\n",
      "step 5400: train loss 3.6877, val loss 4.8978\n",
      "step 5600: train loss 3.6903, val loss 4.9078\n",
      "step 5800: train loss 3.6520, val loss 4.8941\n",
      "step 6000: train loss 3.6436, val loss 4.9562\n",
      "step 6200: train loss 3.6326, val loss 4.9932\n",
      "step 6400: train loss 3.5940, val loss 4.9668\n",
      "step 6600: train loss 3.5735, val loss 4.9790\n",
      "step 6800: train loss 3.5555, val loss 4.9618\n",
      "step 7000: train loss 3.5543, val loss 5.0456\n",
      "step 7200: train loss 3.5201, val loss 5.0090\n",
      "step 7400: train loss 3.5281, val loss 4.9869\n",
      "step 7600: train loss 3.4988, val loss 5.0574\n",
      "step 7800: train loss 3.4686, val loss 5.0086\n",
      "step 8000: train loss 3.4766, val loss 5.0561\n",
      "step 8200: train loss 3.4492, val loss 5.0713\n",
      "step 8400: train loss 3.4264, val loss 5.0913\n",
      "step 8600: train loss 3.4002, val loss 5.0904\n",
      "step 8800: train loss 3.4144, val loss 5.1183\n",
      "step 9000: train loss 3.3656, val loss 5.0851\n",
      "step 9200: train loss 3.3692, val loss 5.1056\n",
      "step 9400: train loss 3.3716, val loss 5.1283\n",
      "step 9600: train loss 3.3450, val loss 5.1280\n",
      "step 9800: train loss 3.3350, val loss 5.1652\n",
      "step 10000: train loss 3.3211, val loss 5.1367\n",
      "step 10200: train loss 3.3072, val loss 5.1808\n",
      "step 10400: train loss 3.3083, val loss 5.1822\n",
      "step 10600: train loss 3.2767, val loss 5.1711\n",
      "step 10800: train loss 3.2662, val loss 5.2038\n",
      "step 11000: train loss 3.2853, val loss 5.2344\n",
      "step 11200: train loss 3.2446, val loss 5.1916\n",
      "step 11400: train loss 3.2225, val loss 5.2604\n",
      "step 11600: train loss 3.2173, val loss 5.2101\n",
      "step 11800: train loss 3.2190, val loss 5.2485\n",
      "step 12000: train loss 3.2061, val loss 5.2742\n",
      "step 12200: train loss 3.1838, val loss 5.2945\n",
      "step 12400: train loss 3.1617, val loss 5.2853\n",
      "step 12600: train loss 3.1832, val loss 5.3302\n",
      "step 12800: train loss 3.1628, val loss 5.3540\n",
      "step 13000: train loss 3.1380, val loss 5.3515\n",
      "step 13200: train loss 3.1162, val loss 5.3281\n",
      "step 13400: train loss 3.1386, val loss 5.3306\n",
      "step 13600: train loss 3.1060, val loss 5.2998\n",
      "step 13800: train loss 3.1056, val loss 5.3651\n",
      "step 14000: train loss 3.0958, val loss 5.3910\n",
      "step 14200: train loss 3.0825, val loss 5.4089\n",
      "step 14400: train loss 3.0874, val loss 5.4349\n",
      "step 14600: train loss 3.0762, val loss 5.4077\n",
      "step 14800: train loss 3.0582, val loss 5.4385\n",
      "step 15000: train loss 3.0620, val loss 5.4270\n",
      "step 15200: train loss 3.0241, val loss 5.4706\n",
      "step 15400: train loss 3.0337, val loss 5.4522\n",
      "step 15600: train loss 3.0162, val loss 5.4515\n",
      "step 15800: train loss 3.0121, val loss 5.4728\n",
      "step 16000: train loss 3.0229, val loss 5.4478\n",
      "step 16200: train loss 3.0018, val loss 5.4965\n",
      "step 16400: train loss 2.9880, val loss 5.5204\n",
      "step 16600: train loss 2.9834, val loss 5.5499\n",
      "step 16800: train loss 2.9807, val loss 5.5343\n",
      "step 17000: train loss 2.9609, val loss 5.5170\n",
      "step 17200: train loss 2.9477, val loss 5.5446\n",
      "step 17400: train loss 2.9593, val loss 5.6256\n",
      "step 17600: train loss 2.9266, val loss 5.6136\n",
      "step 17800: train loss 2.9257, val loss 5.5582\n",
      "step 18000: train loss 2.9230, val loss 5.5307\n",
      "step 18200: train loss 2.9160, val loss 5.5877\n",
      "step 18400: train loss 2.9006, val loss 5.5999\n",
      "step 18600: train loss 2.8934, val loss 5.6309\n",
      "step 18800: train loss 2.8980, val loss 5.6110\n",
      "step 19000: train loss 2.8835, val loss 5.6273\n",
      "step 19200: train loss 2.8706, val loss 5.6514\n",
      "step 19400: train loss 2.8682, val loss 5.6764\n",
      "step 19600: train loss 2.8594, val loss 5.6279\n",
      "step 19800: train loss 2.8606, val loss 5.6966\n",
      "! what noise?\n",
      "\n",
      "Nurse:\n",
      "I think so: I'll bear her; and withRa hoodles.\n",
      "\n",
      "JULIET:\n",
      "O, I'll tell thee what I speak.\n",
      "\n",
      "Nurse:\n",
      "Not here undone, my lord?\n",
      "\n",
      "ROMEO:\n",
      "A torch hath congeal'd hands upon our foes;\n",
      "And every tongue cleft of death hath clps\n",
      "And pay with a thousand crown with kin,\n",
      "But death be unkind; and, in despite of all,\n",
      "But in thy force unto the warlike mind.\n",
      "My wretched boy; and this is the king's knife?\n",
      "Why, how canst thou love the holy mother?\n",
      "\n",
      "Messenger:\n",
      "And see thy time with all str Redeem.\n",
      "\n",
      "GLOUCESTER:\n",
      "Who say you will, give me leave to die;\n",
      "An old brother, we will,\n",
      "If they may drink his father be to thine.\n",
      "\n",
      "PAULINA:\n",
      "I know not, my lord;\n",
      "I thank you this news so.\n",
      "\n",
      "LEONTES:\n",
      "O my desert!\n",
      "\n",
      "PAULINA:\n",
      "And I will go to.\n",
      "\n",
      "LEONTES:\n",
      "O boy,\n",
      "I do here,\n",
      "I'll follow you as he pass'd,\n",
      "Both on my faith, which shall be thus,\n",
      "Both to the bishop of my foe and sorrow,\n",
      "With whom I would have drown'd on foot from the purpose,\n",
      "Both and his confessor,--therery for anointed eye,\n",
      "Lo, in their water-se, to associate.\n",
      "AwayIO:\n",
      "Then I do what I may, not come;\n",
      "But in the queen, and you are fled to the king;\n",
      "But now, when you have a hareen'd.\n",
      "\n",
      "KING RICHARD III:\n",
      "O, I know not? what, what's o'clock?\n",
      "\n",
      "BALTHASAR:\n",
      "Marry, my lord?\n",
      "\n",
      "KING RICHARD III:\n",
      "'Zounds, like him; but I'll come to you.\n",
      "\n",
      "CAPULET:\n",
      "Hail, ho! what!\n",
      "\n",
      "JULIET:\n",
      "What news, no lords?\n",
      "\n",
      "ROMEO:\n",
      "I will, in despite of her; I'll give him.\n",
      "\n",
      "Nurse:\n",
      "Marry, good sir, or; I thank\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import tiktoken\n",
    "\n",
    "#hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 16 #how many independent sequences will we process in parallel\n",
    "context_size = 32 #maximum context length for predictions\n",
    "max_iters = 20000\n",
    "eval_interval = 200\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 300\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# --- Tokenizer ---\n",
    "\"\"\"\n",
    "Tokenizer Setup using TikToken (GPT-2 BPE)\n",
    "\n",
    "This section initializes the tokenizer and prepares the vocabulary for the language model.\n",
    "\n",
    "- Uses TikToken's GPT-2 Byte-Pair Encoding (BPE) tokenizer.\n",
    "- Provides a fast, subword-level tokenization strategy.\n",
    "- Ensures compatibility with GPT-style token indices (n_vocab = 50257).\n",
    "- Defines `encoder` and `decoder` functions to convert between raw text and token ids.\n",
    "- Loads the training corpus from `input.txt`.\n",
    "\n",
    "Variables:\n",
    "    enc (tiktoken.Encoding): GPT-2 BPE tokenizer.\n",
    "    vocab_size (int): Size of tokenizer vocabulary.\n",
    "    encoder (Callable): Function to encode raw text into token ids.\n",
    "    decoder (Callable): Function to decode token ids back into text.\n",
    "    text (str): Entire input corpus read from file.\n",
    "\"\"\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = enc.n_vocab\n",
    "encoder = lambda s: enc.encode(s)\n",
    "decoder = lambda l: enc.decode(l)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#all the unique words occurs in this text\n",
    "# words = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", text)\n",
    "# vocab_size = len(words)\n",
    "# vocab = sorted(set(words)) #building vocab from words\n",
    "# vocab.append('<unk>')\n",
    "# word_to_num_index = {w:i for i, w in enumerate(vocab)}#converting the words to numbers having index\n",
    "# index_num_to_word = {i:w for i, w in enumerate(vocab)} #vice-versa\n",
    "\n",
    "\n",
    "# #safe encoder with fallback to <unk>\n",
    "# encoder = lambda e: [word_to_num_index.get(w, word_to_num_index['<unk>']) for w in re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", e.lower())]\n",
    "\n",
    "# #the decoder\n",
    "# decoder = lambda d: ' '.join([index_num_to_word[i] for i in d])\n",
    "\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "\"\"\"\n",
    "Data Splitting for Language Modeling\n",
    "\n",
    "- Encodes the entire text corpus into token IDs using the GPT-2 BPE tokenizer.\n",
    "- Converts the encoded token list into a PyTorch tensor of type `torch.long`.\n",
    "- Splits the data into training and validation sets (90% / 10%).\n",
    "\n",
    "Variables:\n",
    "    data (Tensor): Full dataset as a 1D tensor of token IDs.\n",
    "    train_data (Tensor): First 90% of `data` used for training.\n",
    "    val_data (Tensor): Remaining 10% of `data` used for validation.\n",
    "\"\"\"\n",
    "data = torch.tensor(encoder(text), dtype=torch.long) #encoding the whole dataset\n",
    "n = int(0.9*len(data)) #first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# --- Data Loader ---\n",
    "\"\"\"\n",
    "get_batch(split)\n",
    "\n",
    "Generates a mini-batch of input (x) and target (y) sequences for training or validation.\n",
    "\n",
    "Each batch consists of `batch_size` sequences of length `context_size`.\n",
    "\n",
    "Args:\n",
    "    split (str): One of 'train' or 'val' to determine the dataset source.\n",
    "\n",
    "Returns:\n",
    "    x (Tensor): Input tensor of shape (batch_size, context_size), representing context tokens.\n",
    "    y (Tensor): Target tensor of shape (batch_size, context_size), representing the next tokens to predict.\n",
    "\"\"\"\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimates the average loss on both training and validation sets.\n",
    "\n",
    "    - Uses `eval_iters` iterations per split to compute a stable loss estimate.\n",
    "    - Runs in evaluation mode with gradient computation disabled (`@torch.no_grad`).\n",
    "    - Returns a dictionary with the mean training and validation losses.\n",
    "\n",
    "    Returns:\n",
    "        out (dict): {\n",
    "            'train': Mean training loss over `eval_iters` batches,\n",
    "            'val': Mean validation loss over `eval_iters` batches\n",
    "        }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Head of Causal Self-Attention.\n",
    "\n",
    "    This module computes attention scores using the query, key, and value \n",
    "    projections of the input, masks out future positions (causal masking), \n",
    "    and performs a weighted aggregation of the values.\n",
    "\n",
    "    Args:\n",
    "        head_size (int): The dimensionality of the query/key/value vectors for this head.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Forward pass for a single attention head.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, T, C), where\n",
    "                        B = batch size,\n",
    "                        T = sequence length(context size),\n",
    "                        C = embedding dimension(feature_dimension).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (B, T, head_size) after applying\n",
    "                    masked scaled dot-product attention.\n",
    "        \"\"\"\n",
    "        B,T,C = x.shape #batch,time,channel basically batch, context size, feature dimersion\n",
    "        k = self.key(x) #(B,T,C)\n",
    "        q = self.query(x) #(B,T,C)\n",
    "        # computing the attention scores (affinities)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 #B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) #(B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform weighted aggregation of the values\n",
    "        v = self.value(x) #(B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention Layer\n",
    "\n",
    "    This module runs multiple self-attention heads in parallel,\n",
    "    allowing the model to attend to different aspects of the input\n",
    "    simultaneously (e.g., short-term, long-term, syntactic, semantic).\n",
    "\n",
    "    It then concatenates their outputs and projects them back into the\n",
    "    original embedding space to be used downstream.\n",
    "\n",
    "    Args:\n",
    "        num_heads (int): Number of parallel attention heads.\n",
    "        head_size (int): Dimensionality of each individual head's key/query/value vectors.\n",
    "\n",
    "    Input Shape:\n",
    "        x: (B, T, n_embd)\n",
    "\n",
    "    Output Shape:\n",
    "        (B, T, n_embd)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # Linear projection to bring concatenated output back to embedding dimension\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head self-attention.\n",
    "\n",
    "        Each head attends to the input independently, producing different views \n",
    "        of the sequence. The outputs from all heads are concatenated and \n",
    "        projected back into the original embedding space.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, T, n_embd), where\n",
    "                        B = batch size,\n",
    "                        T = sequence length,\n",
    "                        n_embd = embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (B, T, n_embd), same as input shape,\n",
    "                    but enriched with attention-based representations.\n",
    "        \"\"\"\n",
    "        # Run all attention heads in parallel and concatenate their outputs\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, num_heads * head_size)\n",
    "\n",
    "        # Project back to the original embedding dimension and apply dropout\n",
    "        out = self.dropout(self.proj(out))  # (B, T, n_embd)\n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward Neural Network (Position-wise MLP)\n",
    "\n",
    "    This module implements the feedforward component of a Transformer block.\n",
    "    It consists of two linear transformations with a ReLU activation in between,\n",
    "    followed by dropout for regularization.\n",
    "\n",
    "    The first linear layer expands the dimensionality from n_embd to 4 * n_embd,\n",
    "    enabling the model to capture more complex patterns. The second layer projects\n",
    "    it back to the original embedding size to maintain consistent dimensions.\n",
    "\n",
    "    Args:\n",
    "        n_embd (int): The dimensionality of the input and output embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Forward pass of the feedforward network.\n",
    "\n",
    "        Applies a two-layer MLP with ReLU activation in between. The input is first\n",
    "        projected to a higher-dimensional space (4x the embedding size), non-linearly\n",
    "        transformed, then projected back to the original embedding size. A dropout \n",
    "        is applied at the end for regularization.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, T, n_embd), where\n",
    "                        B = batch size,\n",
    "                        T = sequence length(context size),\n",
    "                        n_embd = embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (B, T, n_embd).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block: Communication (Self-Attention) + Computation (Feedforward)\n",
    "\n",
    "    This block forms the basic unit of a Transformer model. It contains two main components:\n",
    "    1. A Multi-Head Self-Attention mechanism that allows the model to attend to different parts \n",
    "       of the sequence (communication).\n",
    "    2. A Feedforward Neural Network (position-wise MLP) for nonlinear transformation (computation).\n",
    "\n",
    "    Both components are preceded by Layer Normalization and employ residual connections \n",
    "    for stable training and better gradient flow.\n",
    "\n",
    "    Args:\n",
    "        n_embd (int): The dimensionality of the token embeddings.\n",
    "        n_head (int): The number of attention heads to use in the multi-head attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) #multi-head self-attention layer\n",
    "        self.ffn = FeedForwardNet(n_embd) #feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd) #layer normalization before self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd) #layer normalization before feedforward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Applies LayerNorm → Multi-head Self-Attention → Residual Add,\n",
    "        then LayerNorm → FeedForward → Residual Add.\n",
    "\n",
    "        This pattern (Norm → SubLayer → Residual) is known as \"Pre-Norm\" and \n",
    "        helps with training stability.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, T, n_embd), where\n",
    "                        B = batch size,\n",
    "                        T = sequence length,\n",
    "                        n_embd = embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (B, T, n_embd).\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.ln1(x)) #communication (multihead-attention)\n",
    "        x = x + self.ffn(self.ln2(x)) #computation (feedforward-network)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Bigram Language Model\n",
    "\n",
    "    This model learns to predict the next token in a sequence given a context of previous tokens.\n",
    "    It is built using:\n",
    "    - Token and positional embeddings\n",
    "    - A stack of Transformer blocks (self-attention + feedforward)\n",
    "    - A linear head to project hidden states into vocabulary logits\n",
    "\n",
    "    While it's called a \"bigram\" model, this implementation uses a full transformer stack\n",
    "    and learns from longer context windows (`context_size` tokens).\n",
    "\n",
    "    Architecture:\n",
    "        Input -> [Token + Positional Embedding] -> [Transformer Blocks] -> [LayerNorm] -> [Linear Head] -> Output logits\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding (nn.Embedding): Maps token indices to embedding vectors.\n",
    "        position_embedding_table (nn.Embedding): Provides positional encodings to preserve token order.\n",
    "        blocks (nn.Sequential): Stack of `Block` layers (Transformer blocks).\n",
    "        ln_f (nn.LayerNorm): Final layer normalization.\n",
    "        lm_head (nn.Linear): Maps final hidden state to vocabulary logits.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            idx (Tensor): Input tensor of token indices with shape (B, T)\n",
    "                          B = batch size, T = context window length.\n",
    "            targets (Tensor, optional): Ground-truth next token indices for loss computation.\n",
    "                                        Shape should be (B, T). If None, loss is not computed.\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Raw prediction scores for the next token (B, T, vocab_size)\n",
    "            loss (Tensor or None): Cross-entropy loss between predictions and targets, if targets is provided.\n",
    "        \"\"\"\n",
    "        B,T = idx.shape\n",
    "\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively from the model.\n",
    "\n",
    "        At each step, the model:\n",
    "        - Conditions on the last `context_size` tokens\n",
    "        - Predicts the next token\n",
    "        - Samples from the logits with optional temperature scaling and top-k filtering\n",
    "\n",
    "        Args:\n",
    "            idx (Tensor): Initial sequence of token indices of shape (B, T).\n",
    "            max_new_tokens (int): Number of tokens to generate.\n",
    "            temperature (float): Softens the logits; lower = more confident, higher = more random.\n",
    "            top_k (int or None): If set, limits sampling to top-k most probable tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Sequence of token indices of shape (B, T + max_new_tokens).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "    \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "    \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "\n",
    "# --- Train the model ---\n",
    "model = BigramLanguageModel().to(device)\n",
    "\n",
    "# Print total number of model parameters in millions\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ========================\n",
    "# Main Training Loop\n",
    "# ========================\n",
    "\"\"\"\n",
    "Trains the language model over `max_iters` iterations.\n",
    "\n",
    "At each iteration:\n",
    "- A batch of input and target sequences is sampled from training data\n",
    "- Forward pass computes predictions and loss\n",
    "- Backpropagation updates model weights using AdamW optimizer\n",
    "\n",
    "Every `eval_interval` steps:\n",
    "- Estimates average train and validation loss over `eval_iters` batches\n",
    "- Prints the current step and corresponding losses\n",
    "\"\"\"\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Fetch training batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Compute loss and update weights\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# --- Generate output ---\n",
    "\"\"\"\n",
    "Generates text from the trained model using autoregressive sampling.\n",
    "\n",
    "Steps:\n",
    "- Starts with a single zero-token (`context`)\n",
    "- At each step, predicts the next token using the model\n",
    "- Applies temperature and top-k sampling to increase generation quality\n",
    "- Decodes the final token sequence into human-readable text\n",
    "\"\"\"\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Start token (B=1, T=1)\n",
    "generated_ids = model.generate(context, max_new_tokens=500, temperature=0.8, top_k=40)\n",
    "print(decoder(generated_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T18:18:15.308167Z",
     "iopub.status.busy": "2025-06-22T18:18:15.307559Z",
     "iopub.status.idle": "2025-06-22T18:18:15.380107Z",
     "shell.execute_reply": "2025-06-22T18:18:15.379572Z",
     "shell.execute_reply.started": "2025-06-22T18:18:15.308144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt1_from_scratch.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T18:18:49.377704Z",
     "iopub.status.busy": "2025-06-22T18:18:49.377147Z",
     "iopub.status.idle": "2025-06-22T18:18:49.496503Z",
     "shell.execute_reply": "2025-06-22T18:18:49.495791Z",
     "shell.execute_reply.started": "2025-06-22T18:18:49.377680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding): Embedding(50257, 64)\n",
       "  (position_embedding_table): Embedding(32, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForwardNet(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForwardNet(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForwardNet(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForwardNet(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramLanguageModel().to(device)\n",
    "model.load_state_dict(torch.load('gpt1_from_scratch.pt'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
